{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c73ddc1",
   "metadata": {},
   "source": [
    "![Autodistill banner](https://camo.githubusercontent.com/2a4712931b5698aca7a95dfd90eb56a693fe59320eee64c1897042d5c3c21959/68747470733a2f2f6d656469612e726f626f666c6f772e636f6d2f6f70656e2d736f757263652f6175746f64697374696c6c2f6175746f64697374696c6c2d62616e6e65722e706e67)\n",
    "\n",
    "# Create a Shipping Container Detection Model Without Labelling\n",
    "\n",
    "In this notebook, we're going to create a model that can detect shipping containers. We will employ a technique called distillation, where we use the knowledge in one model -- Grounding DINO, a zero-shot object detection model -- to build a smaller model that is tuned to a specific use case.\n",
    "\n",
    "By distilling Grounding DINO, we can create a new model that is smaller, more performant in terms of inference time, and easier to tune because the model is smaller and we have the full underlying dataset used to train our model.\n",
    "\n",
    "To distill the knowledge from Grounding DINO into a smaller model specifically for detecting shipping containers, we will use `autodistill`, a new Python package for distilling large computer vision models into smaller ones. We'll train a new model using YOLOv8 and our labelled images.\n",
    "\n",
    "By the end of this guide, we'll have a shipping container detection model that we can run on images and videos.\n",
    "\n",
    "Without further ado, let's start coding!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2d5bd94",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install and import the dependencies required for this project and set the requisite configuration values for our project. We'll also load the shipping container dataset for use in our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd4207b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q autodistill autodistill-grounded-sam autodistill-yolov8 supervision\n",
    "!wget https://autodistill.s3.amazonaws.com/containers.zip\n",
    "!unzip -q containers.zip -d containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "284d8f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autodistill_grounded_sam import GroundedSAM\n",
    "from autodistill.detection import CaptionOntology\n",
    "from autodistill_yolov8 import YOLOv8\n",
    "import supervision as sv\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "HOME = os.getcwd()\n",
    "DATA = HOME + \"/data/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce1dd84a",
   "metadata": {},
   "source": [
    "## Label the Container Dataset with Grounding DINO\n",
    "\n",
    "Now that we have installed the required dependencies and configured our project, we're ready to start building our model.\n",
    "\n",
    "The first step is to label all the cars in the container images in our dataset. To do so, we'll leverage `autodistill` to programatically label images using Grounding DINO.\n",
    "\n",
    "The steps we'll follow are:\n",
    "\n",
    "1. Create an ontology for labelling images\n",
    "2. Install Grounding DINO\n",
    "3. Run inference on Grounding DINO using images from our dataset\n",
    "\n",
    "This will give us a folder of annotated images we can use to train a YOLOv8 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e019978",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = GroundedSAM(ontology=CaptionOntology({\"shipping container\": \"container\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da21a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label all of our images\n",
    "base_model.label(\"./context_images\", extension=\".jpeg\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d1a852a",
   "metadata": {},
   "source": [
    "## Train a YOLOv8 Model\n",
    "\n",
    "Next, we'll use our labelled images to train a YOLOv8 model to detect shipping containers. To train the model, we'll use the Python methods available in the `ultralytics` pip package. To learn more about training your own YOLOv8 model, check out our [How to Train a YOLOv8 Object Detection Model notebook](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-object-detection-on-custom-dataset.ipynb).\n",
    "\n",
    "We'll save our weights to disk so you can use your weights wherever you would like to deploy your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f34106",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = YOLOv8(\"yolov8n.pt\")\n",
    "\n",
    "target_model.train(\"./containers_labeled/data.yaml\", epochs=200)\n",
    "\n",
    "# export weights for future use\n",
    "saved_weights = target_model.export(format=\"onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02060f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics for your model\n",
    "metrics = target_model.val()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd2319bd",
   "metadata": {},
   "source": [
    "## Test the Model\n",
    "\n",
    "With a trained model ready, we can run inference on a few example images of containers to see how our model performs. Let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662e8347",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image = \"./containers_labeled/train/images/0c59799a5b2b95c8b53df911264676f71b0e59c8_jpg.rf.c7af7b3a1332460b5495be354468fd65.jpg\"\n",
    "\n",
    "pred = target_model.predict(example_image, conf=0.01)\n",
    "\n",
    "res_plotted = pred[0].plot()\n",
    "image_rgb = cv2.cvtColor(res_plotted, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "sv.plot_image(image_rgb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1d146ca",
   "metadata": {},
   "source": [
    "## (Optional) Run Inference on a Video\n",
    "\n",
    "Using `supervision`, we can run inference on a video to detect containers throughout the video. In the code below, we'll use a video of shipping containers and annotate all containers. We'll also show a count of the number of containers present in each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4384ddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "import numpy as np\n",
    "\n",
    "box_annotator = sv.BoxAnnotator(thickness=2, text_thickness=4, text_scale=2)\n",
    "\n",
    "# classes are taken from the class name values in the ontology we specified earlier\n",
    "classes = \"container\"\n",
    "\n",
    "VIDEO = f\"{HOME}/video.mp4\"\n",
    "OUTPUT = f\"{HOME}/output.mp4\"\n",
    "\n",
    "def process_frame(frame: np.ndarray, i: int) -> np.ndarray:\n",
    "    print(f\"Processing frame {i}\")\n",
    "    predictions = target_model(frame).json()\n",
    "    detections = sv.Detections.from_yolov8(roboflow_result = predictions, class_list = classes)\n",
    "    detections = detections[(detections.confidence > 0.3)]\n",
    "\n",
    "    labels = [\n",
    "        f\"{classes[class_id]} {confidence:0.2f}\"\n",
    "        for _, _, confidence, class_id, _\n",
    "        in detections\n",
    "    ]\n",
    "\n",
    "    frame = box_annotator.annotate(scene=frame, detections=detections, labels=labels)\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b885c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = sv.get_video_frames_generator(VIDEO)\n",
    "\n",
    "sv.process_video(\n",
    "    source_path=VIDEO,\n",
    "    target_path=OUTPUT,\n",
    "    callback=lambda frame, i: process_frame(frame, i)\n",
    ")\n",
    "\n",
    "print(f\"Output saved to {OUTPUT}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eda5c84e",
   "metadata": {},
   "source": [
    "Open the video you saved to see the result of the model on the video."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3400b693",
   "metadata": {},
   "source": [
    "## üèÜ Congratulations\n",
    "\n",
    "### Learning Resources\n",
    "\n",
    "Roboflow has produced many resources that you may find interesting as you advance your knowledge of computer vision:\n",
    "\n",
    "- [Roboflow Notebooks](https://github.com/roboflow/notebooks): A repository of over 20 notebooks that walk through how to train custom models with a range of model types, from YOLOv7 to SegFormer.\n",
    "- [Roboflow YouTube](https://www.youtube.com/c/Roboflow): Our library of videos featuring deep dives into the latest in computer vision, detailed tutorials that accompany our notebooks, and more.\n",
    "- [Roboflow Discuss](https://discuss.roboflow.com/): Have a question about how to do something on Roboflow? Ask your question on our discussion forum.\n",
    "- [Roboflow Models](https://roboflow.com/models): Learn about state-of-the-art models and their performance. Find links and tutorials to guide your learning.\n",
    "\n",
    "### Convert data formats\n",
    "\n",
    "Roboflow provides free utilities to convert data between dozens of popular computer vision formats. Check out [Roboflow Formats](https://roboflow.com/formats) to find tutorials on how to convert data between formats in a few clicks.\n",
    "\n",
    "### Connect computer vision to your project logic\n",
    "\n",
    "[Roboflow Templates](https://roboflow.com/templates) is a public gallery of code snippets that you can use to connect computer vision to your project logic. Code snippets range from sending emails after inference to measuring object distance between detections."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
